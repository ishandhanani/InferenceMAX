dsr1-fp4-b200-sglang:
  image: lmsysorg/sglang:v0.5.5-cu129-amd64
  model: nvidia/DeepSeek-R1-0528-FP4-V2
  model-prefix: dsr1
  runner: b200
  precision: fp4
  framework: sglang
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 4, ep: 4, conc-start: 4, conc-end: 128 }
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 128 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 4, ep: 4, conc-start: 4, conc-end: 128 }
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 128 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 4, ep: 4, conc-start: 4, conc-end: 128 }
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 16 }

dsr1-fp4-b200-trt:
  image: nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2
  model: nvidia/DeepSeek-R1-0528-FP4-V2
  model-prefix: dsr1
  runner: b200-trt
  precision: fp4
  framework: trt
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    # If TP=4, 
    #   If CONC > 32, then EP=4
    #   If CONC >= 256, DP_ATTN=true
    - { tp: 4, conc-start: 4, conc-end: 32 }
    - { tp: 4, ep: 4, conc-start: 64, conc-end: 128 }
    - { tp: 4, ep: 4, dp-attn: true, conc-start: 256, conc-end: 256 }
    # If TP=8, 
    #   If CONC > 8, then EP=8
    #   If CONC >= 256, DP_ATTN=true
    - { tp: 8, conc-start: 4, conc-end: 8 }
    - { tp: 8, ep: 8, conc-start: 16, conc-end: 128 }
    - { tp: 8, ep: 8, dp-attn: true, conc-start: 256, conc-end: 256 }
  - isl: 1024
    osl: 8192
    search-space:
    # If TP=4, 
    #   If CONC > 32, then EP=4
    #   If CONC >= 256, DP_ATTN=true
    - { tp: 4, conc-start: 4, conc-end: 32 }
    - { tp: 4, ep: 4, conc-start: 64, conc-end: 128 }
    - { tp: 4, ep: 4, dp-attn: true, conc-start: 256, conc-end: 256 }
    # If TP=8, 
    #   If CONC > 16, then EP=8
    #   If CONC >= 256, DP_ATTN=true
    - { tp: 8, conc-start: 4, conc-end: 16 }
    - { tp: 8, ep: 8, conc-start: 32, conc-end: 128 }
    - { tp: 8, ep: 8, dp-attn: true, conc-start: 256, conc-end: 256 }
  - isl: 8192
    osl: 1024
    search-space:
    # If TP=4, 
    #   If CONC > 32, then EP=4 and DP_ATTN=true
    - { tp: 4, ep: 4, conc-start: 4, conc-end: 32 }
    - { tp: 4, ep: 4, dp-attn: true, conc-start: 64, conc-end: 256 }
    # If TP=8, 
    #   If CONC > 32, then EP=8 and DP_ATTN=true
    - { tp: 8, conc-start: 4, conc-end: 32 }
    - { tp: 8, ep: 8, dp-attn: true, conc-start: 64, conc-end: 256 }

dsr1-fp8-b200-sglang:
  image: lmsysorg/sglang:v0.5.5-cu129-amd64
  model: deepseek-ai/DeepSeek-R1-0528
  model-prefix: dsr1
  runner: b200
  precision: fp8
  framework: sglang
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 8, ep: 1, conc-start: 4, conc-end: 64 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 8, ep: 1, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 8, ep: 1, conc-start: 4, conc-end: 64 }

dsr1-fp8-b200-trt:
  image: nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2
  model: deepseek-ai/DeepSeek-R1-0528
  model-prefix: dsr1
  runner: b200-trt
  precision: fp8
  framework: trt
  multinode: false
  seq-len-configs:
  # For all sequence lengths, EP=TP
  - isl: 1024
    osl: 1024
    search-space:
    # If CONC > 32, then DP_ATTN=true
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 32 }
    - { tp: 8, ep: 8, dp-attn: true, conc-start: 64, conc-end: 64 }
  - isl: 1024
    osl: 8192
    search-space:
    # If CONC > 64, then DP_ATTN=true
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    search-space:
    # If CONC > 64, then DP_ATTN=true
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 64 }

dsr1-fp8-h200-sglang:
  image: lmsysorg/sglang:v0.5.5-cu129-amd64
  model: deepseek-ai/DeepSeek-R1-0528
  model-prefix: dsr1
  runner: h200
  precision: fp8
  framework: sglang
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 8, conc-start: 4, conc-end: 64 }

dsr1-fp8-h200-trt:
  image: nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2
  model: deepseek-ai/DeepSeek-R1-0528
  model-prefix: dsr1
  runner: h200
  precision: fp8
  framework: trt
  multinode: false
  # For all sequence lengths, EP=TP
  seq-len-configs:
  - isl: 1024
    osl: 1024
    # If CONC > 64, then DP_ATTN=true
    search-space:
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 64 }
  - isl: 1024
    osl: 8192
    # If CONC > 64, then DP_ATTN=true
    search-space:
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    # If CONC > 32, then DP_ATTN=true
    search-space:
    - { tp: 8, ep: 8, conc-start: 4, conc-end: 32 }
    - { tp: 8, ep: 8, dp-attn: true, conc-start: 64, conc-end: 64 }

gptoss-fp4-b200-trt:
  image: nvcr.io#nvidia/tensorrt-llm/release:1.2.0rc2
  model: openai/gpt-oss-120b
  model-prefix: gptoss
  runner: b200-trt
  precision: fp4
  framework: trt
  multinode: false
  # For all sequence lengths, if CONC >= 256, then EP=TP and DP_ATTN=true
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 2, dp-attn: true, conc-start: 32, conc-end: 128 }
    - { tp: 4, dp-attn: true, conc-start: 32, conc-end: 64 }
    - { tp: 1, conc-start: 64, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 32 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 8 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 1, conc-start: 64, conc-end: 128 }
    - { tp: 2, dp-attn: true, conc-start: 64, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 128 }
    - { tp: 4, conc-start: 4, conc-end: 128 }
    - { tp: 8, conc-start: 4, conc-end: 16 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 1, conc-start: 64, conc-end: 128 }
    - { tp: 2, dp-attn: true, conc-start: 64, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 128 }
    - { tp: 4, conc-start: 4, conc-end: 32 }
    - { tp: 8, conc-start: 4, conc-end: 8 }

gptoss-fp4-b200-vllm:
  image: vllm/vllm-openai:v0.11.0
  model: openai/gpt-oss-120b
  model-prefix: gptoss
  runner: b200
  precision: fp4
  framework: vllm
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 128 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 8 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 128 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 8 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 128 }
    - { tp: 2, conc-start: 4, conc-end: 128 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 4 }

gptoss-fp4-h100-vllm:
  image: vllm/vllm-openai:v0.11.0
  model: openai/gpt-oss-120b
  model-prefix: gptoss
  runner: h100
  precision: fp4
  framework: vllm
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 16 }

gptoss-fp4-h200-trt:
  image: nvcr.io#nvidia/tensorrt-llm/release:gpt-oss-dev
  model: openai/gpt-oss-120b
  model-prefix: gptoss
  runner: h200
  precision: fp4
  framework: trt
  multinode: false
  # For all sequence lengths, EP=TP, DP_ATTENTION=false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 1, ep: 1, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 2, ep: 2, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 4, ep: 4, dp-attn: false, conc-start: 4, conc-end: 32 }
    - { tp: 8, ep: 8, dp-attn: false, conc-start: 4, conc-end: 8 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 1, ep: 1, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 2, ep: 2, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 4, ep: 4, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 8, ep: 8, dp-attn: false, conc-start: 4, conc-end: 8 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 1, ep: 1, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 2, ep: 2, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 4, ep: 4, dp-attn: false, conc-start: 4, conc-end: 64 }
    - { tp: 8, ep: 8, dp-attn: false, conc-start: 4, conc-end: 8 }

gptoss-fp4-h200-vllm:
  image: vllm/vllm-openai:v0.11.0
  model: openai/gpt-oss-120b
  model-prefix: gptoss
  runner: h200
  precision: fp4
  framework: vllm
  multinode: false
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 4 }
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 1024
    osl: 8192
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 4 }
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 64 }
  - isl: 8192
    osl: 1024
    search-space:
    - { tp: 1, conc-start: 4, conc-end: 64 }
    - { tp: 2, conc-start: 4, conc-end: 64 }
    - { tp: 4, conc-start: 4, conc-end: 64 }
    - { tp: 8, conc-start: 4, conc-end: 32 }

dsr1-fp4-gb200-dynamo-trt:
  image: nvcr.io#nvidia/ai-dynamo/tensorrtllm-runtime:0.5.1-rc0.pre3
  model: deepseek-r1-fp4
  model-prefix: dsr1
  runner: gb200
  precision: fp4
  framework: dynamo-trt
  multinode: true
  disagg: true
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    # MTP configurations
    # tep - Run Tensor-Expert Parallel mode (attention_dp=false)
    # NOTE: Prefill tp and ep are always 4 because each GB200 node has 4 GPUs and
    # ctx_tp_size is hardcoded to 4 in launch_gb200-nv.sh. Decode tp/ep matches gen_tp_size.
    # For 1k/1k: prefill batch-size=4, max-num-tokens=4608
    - spec-decoding: "mtp"
      conc-list: [ 1, 2, 4, 8, 16, 36 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: false
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 4
        tp: 8
        ep: 8
        dp-attn: false
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=128"
        - "DECODE_MAX_BATCH_SIZE=32"
        - "DECODE_GPU_MEM_FRACTION=0.9"
        - "DECODE_MTP_SIZE=3"

    # dep - Run Data-Expert Parallel mode (attention_dp=true)
    - spec-decoding: "mtp"
      conc-list: [ 512, 1075 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=256"
        - "DECODE_MAX_BATCH_SIZE=64"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=3"

    - spec-decoding: "mtp"
      conc-list: [ 2150 ]
      prefill:
        num-worker: 2
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=256"
        - "DECODE_MAX_BATCH_SIZE=128"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=1"

    - spec-decoding: "mtp"
      conc-list: [ 512 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 32
        ep: 32
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=64"
        - "DECODE_MAX_BATCH_SIZE=16"
        - "DECODE_GPU_MEM_FRACTION=0.6"
        - "DECODE_MTP_SIZE=3"

    - spec-decoding: "mtp"
      conc-list: [ 2252 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 8
        ep: 8
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=512"
        - "DECODE_MAX_BATCH_SIZE=256"
        - "DECODE_GPU_MEM_FRACTION=0.8"
        - "DECODE_MTP_SIZE=1"

    # Non-MTP configurations (default spec_decoding="none")
    # tep - Run Tensor-Expert Parallel mode (attention_dp=false)
    - conc-list: [ 1, 2, 4, 8, 16, 32, 64, 141 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: false
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 4
        tp: 8
        ep: 8
        dp-attn: false
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=128"
        - "DECODE_MAX_BATCH_SIZE=128"
        - "DECODE_GPU_MEM_FRACTION=0.9"
        - "DECODE_MTP_SIZE=0"

    # dep - Run Data-Expert Parallel mode (attention_dp=true)
    - conc-list: [ 1075 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 32
        ep: 32
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=32"
        - "DECODE_MAX_BATCH_SIZE=32"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 1075 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=64"
        - "DECODE_MAX_BATCH_SIZE=64"
        - "DECODE_GPU_MEM_FRACTION=0.75"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 2048, 4300 ]
      prefill:
        num-worker: 2
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=256"
        - "DECODE_MAX_BATCH_SIZE=256"
        - "DECODE_GPU_MEM_FRACTION=0.75"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 4300 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=4608"
        - "PREFILL_MAX_BATCH_SIZE=4"
      decode:
        num-worker: 1
        tp: 8
        ep: 8
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=512"
        - "DECODE_MAX_BATCH_SIZE=512"
        - "DECODE_GPU_MEM_FRACTION=0.8"
        - "DECODE_MTP_SIZE=0"

  - isl: 8192
    osl: 1024
    search-space:
    # MTP configurations (spec_decoding="mtp")
    # tep - Run Tensor-Expert Parallel mode (attention_dp=false)
    # For 8k/1k: prefill batch-size=1, max-num-tokens=8448
    - spec-decoding: "mtp"
      conc-list: [ 1, 2, 4, 8, 18 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: false
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 3
        tp: 8
        ep: 8
        dp-attn: false
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=64"
        - "DECODE_MAX_BATCH_SIZE=16"
        - "DECODE_GPU_MEM_FRACTION=0.9"
        - "DECODE_MTP_SIZE=3"

    # dep - Run Data-Expert Parallel mode (attention_dp=true)
    - spec-decoding: "mtp"
      conc-list: [ 128, 269 ]
      prefill:
        num-worker: 5
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 32
        ep: 32
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=32"
        - "DECODE_MAX_BATCH_SIZE=8"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=3"

    - spec-decoding: "mtp"
      conc-list: [ 538 ]
      prefill:
        num-worker: 8
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 32
        ep: 32
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=64"
        - "DECODE_MAX_BATCH_SIZE=16"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=3"

    - spec-decoding: "mtp"
      conc-list: [ 1075 ]
      prefill:
        num-worker: 8
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=256"
        - "DECODE_MAX_BATCH_SIZE=64"
        - "DECODE_GPU_MEM_FRACTION=0.75"
        - "DECODE_MTP_SIZE=2"

    - spec-decoding: "mtp"
      conc-list: [ 2150 ]
      prefill:
        num-worker: 6
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 8
        ep: 8
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=512"
        - "DECODE_MAX_BATCH_SIZE=256"
        - "DECODE_GPU_MEM_FRACTION=0.8"
        - "DECODE_MTP_SIZE=1"

    # Non-MTP configurations (default spec_decoding="none")
    # tep - Run Tensor-Expert Parallel mode (attention_dp=false)
    - conc-list: [ 1, 2, 4, 8, 16, 34 ]
      prefill:
        num-worker: 1
        tp: 4
        ep: 4
        dp-attn: false
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 3
        tp: 8
        ep: 8
        dp-attn: false
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=32"
        - "DECODE_MAX_BATCH_SIZE=32"
        - "DECODE_GPU_MEM_FRACTION=0.9"
        - "DECODE_MTP_SIZE=0"

    # dep - Run Data-Expert Parallel mode (attention_dp=true)
    - conc-list: [ 256, 538 ]
      prefill:
        num-worker: 4
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 32
        ep: 32
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=16"
        - "DECODE_MAX_BATCH_SIZE=16"
        - "DECODE_GPU_MEM_FRACTION=0.7"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 1075 ]
      prefill:
        num-worker: 6
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=64"
        - "DECODE_MAX_BATCH_SIZE=64"
        - "DECODE_GPU_MEM_FRACTION=0.75"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 2150 ]
      prefill:
        num-worker: 8
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 16
        ep: 16
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=128"
        - "DECODE_MAX_BATCH_SIZE=128"
        - "DECODE_GPU_MEM_FRACTION=0.75"
        - "DECODE_MTP_SIZE=0"

    - conc-list: [ 2150 ]
      prefill:
        num-worker: 5
        tp: 4
        ep: 4
        dp-attn: true
        additional-settings:
        - "PREFILL_MAX_NUM_TOKENS=8448"
        - "PREFILL_MAX_BATCH_SIZE=1"
      decode:
        num-worker: 1
        tp: 8
        ep: 8
        dp-attn: true
        additional-settings:
        - "DECODE_MAX_NUM_TOKENS=256"
        - "DECODE_MAX_BATCH_SIZE=256"
        - "DECODE_GPU_MEM_FRACTION=0.8"
        - "DECODE_MTP_SIZE=0"

dsr1-fp8-gb200-dynamo-sglang:
  image: nvcr.io/nvidia/ai-dynamo/sglang-runtime:0.5.1-rc0.pre1
  model: deepseek-ai/DeepSeek-R1-0528
  model-prefix: dsr1
  runner: gb200
  precision: fp8
  framework: dynamo-sglang
  multinode: true
  disagg: true
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    # "Top of curve" (2 prefill workers each at DEP8 and 1 decode worker at DEP32)
    - spec-decoding: "none"
      conc-list: [ 4096 ]
      prefill:
        num-worker: 2
        # tp, ep, and dp-attn do nothing because they are hardcoded in the following file:
        # https://github.com/Elnifio/dynamo/blob/update-result-file-name/components/backends/sglang/slurm_jobs/scripts/gb200-fp8.sh
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=4"
        - "N_ADDITIONAL_FRONTENDS=9"
        - "SCRIPT_MODE=max-tpt"
      decode:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=8"

    # "Bottom of curve" (1 prefill worker at DEP4 and 4 decode workers at DEP4)
    - spec-decoding: "none"
      conc-list: [ 2, 4, 8, 16, 64, 128 ]
      prefill:
        num-worker: 1
        # tp, ep, and dp-attn do nothing because they are hardcoded in the following file:
        # https://github.com/Elnifio/dynamo/blob/update-result-file-name/components/backends/sglang/slurm_jobs/scripts/gb200-fp8.sh
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=1"
        - "N_ADDITIONAL_FRONTENDS=9"
        - "SCRIPT_MODE=1p_4d"
      decode:
        num-worker: 4
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=4"

    # "Middle of curve" (3 prefill workers each at DEP8 and 1 decode worker at DEP48)
    - spec-decoding: "none"
      conc-list: [ 1024, 2048, 4096 ]
      prefill:
        num-worker: 3
        # tp, ep, and dp-attn do nothing because they are hardcoded in the following file:
        # https://github.com/Elnifio/dynamo/blob/update-result-file-name/components/backends/sglang/slurm_jobs/scripts/gb200-fp8.sh
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=6"
        - "N_ADDITIONAL_FRONTENDS=9"
      decode:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=12"

  - isl: 8192
    osl: 1024
    search-space:
    # Low latency (1 prefill worker at DEP4 and 1 decode worker at DEP4)
    - spec-decoding: "none"
      conc-list: [ 4, 8, 16, 32, 64, 128, 256, 512 ]
      prefill:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=1"
        - "N_ADDITIONAL_FRONTENDS=8"
      decode:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=1"

    # Middle and top of curve (5 prefill workers each at DEP8 and 1 decode worker at DEP32)
    - spec-decoding: "none"
      conc-list: [ 512, 1024, 2048, 6144 ]
      prefill:
        num-worker: 5
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=2"
        - "N_ADDITIONAL_FRONTENDS=8"
      decode:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=8"

dsr1-fp4-gb200-dynamo-sglang:
  # TODO: swap
  image: nvcr.io/nvidia/ai-dynamo/sglang-runtime:0.5.1-rc0.pre1
  # TODO: what is the right name?
  model: deepseek-ai/DeepSeek-R1-0528-fp4-v2
  model-prefix: dsr1
  runner: gb200
  precision: fp4
  framework: dynamo-sglang
  multinode: true
  disagg: true
  seq-len-configs:
  - isl: 1024
    osl: 1024
    search-space:
    # Low latency (1 prefill worker at DEP4 and 2 decode workers at DEP4)
    - spec-decoding: "none"
      conc-list: [ 4, 8, 32, 64, 128, 112, 128, 256 ]
      prefill:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=1"
        - "N_ADDITIONAL_FRONTENDS=8"
      decode:
        num-worker: 2
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=2"

    # Mid curve (1 prefill worker at DEP4 and 1 decode workers at DEP48)
    - spec-decoding: "none"
      conc-list: [ 512, 1024, 2048, 4096, 8192 ]
      prefill:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=1"
        - "N_ADDITIONAL_FRONTENDS=8"
      decode:
        num-worker: 2
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=12"

    # Top of curve (1 prefill worker at DEP4 and 1 decode worker at DEP32)
    - spec-decoding: "none"
      conc-list: [ 8192, 12000, 15000 ]
      prefill:
        num-worker: 1
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "PREFILL_NODES=1"
        - "N_ADDITIONAL_FRONTENDS=8"
      decode:
        num-worker: 2
        tp: 1
        ep: 1
        dp-attn: true
        additional-settings:
        - "DECODE_NODES=8"